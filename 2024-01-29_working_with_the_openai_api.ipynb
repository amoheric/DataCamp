{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee16a26d5ddd948",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from configparser import ConfigParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dbdfefc0bc510e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "img_path = 'https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/2024-01-29_working_with_the_openai_api'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca0961a-792f-4265-b5d5-57439ab9a85e",
   "metadata": {},
   "source": [
    "# Introduction to the OpenAI API\n",
    "\n",
    "Harness the power of AI from OpenAI's models by creating requests to their API with just a few lines of code. Discover the wide range of capabilities available via the OpenAI API. Learn about the best practices for managing API usage across the business by utilizing API organizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecfb999-f2b3-4d97-81c1-ac5596b9e007",
   "metadata": {},
   "source": [
    "## What is the OpenAI API\n",
    "\n",
    "1. What is the OpenAI API?: Welcome to this course! I'm James, and I'll be your host as we explore the artificial intelligence functionality available via the OpenAI API!\n",
    "\n",
    "2. Coming up...: In this course, you'll learn how to use the AI models available through the OpenAI API to solve a wide range of real-world tasks. To do this, we'll be using Python code throughout the course, and expect familiarity with a few Python programming topics like subsetting lists and dictionaries, control flow, and looping. However, no experience with AI or machine learning is required. With that, let's dive right in!\n",
    "- **Course goals**\n",
    "  - Use the OpenAI API to access AI models\n",
    "  - Use those models to solve various tasks\n",
    "  - Use Python code to do this!\n",
    "\n",
    "- **Expected knowledge**\n",
    "  - Subsetting Python lists and dictionaries\n",
    "  - Control flow: `if`, `elif`, `else`\n",
    "  - Loops: `for`, `while`\n",
    "\n",
    "- **Not expected to know**\n",
    "  - AI or machine learning\n",
    "\n",
    "3. OpenAI, ChatGPT, and the OpenAI API: OpenAI is a company that researches and develops artificial intelligence systems. One of their most famous developments is ChatGPT, which is an application that allows users to communicate with an AI-powered chatbot to ask questions, perform tasks, or generate content. The OpenAI API allows individuals or organizations to access and customize any of the models developed and released by OpenAI. If OpenAI was a car manufacturer, ChatGPT would be their shiny new sports car that people can walk into a dealership and test drive. The OpenAI API would be like the system customers could use to customize and order any car from the manufacturer's catalog.\n",
    "- **OpenAI**: AI R&D company\n",
    "- **ChatGPT**: AI application\n",
    "- **OpenAI API**: Interface for accessing OpenAI models\n",
    "\n",
    "4. What is an API?: So what actually is an API? API stands for Application Programming Interface, and they act as a messenger between software applications, taking a request to a system and receiving a response containing data or services. An API is like a waiter in a restaurant; they take our order, or request, communicate it to the kitchen—the system providing the service—and finally, deliver the food, or response from the system, back to our table. Many applications interact using APIs; for example, a mobile weather app may send our location to an API and request the local forecast, which gets returned to our phones.\n",
    "\n",
    "5. The OpenAI API: We can similarly write code to interact with the OpenAI API and request the use of one of their models. Our request, in this case, will specify which model we want, the data that we want the model to use, and any other parameters to customize the model's behavior. The response, containing the model result, is then returned to us.\n",
    "\n",
    "6. API vs. web interface: Some of OpenAI's models, such as ChatGPT, can be used from the web browser, so what are the benefits of accessing them via the API? If we're looking to streamline our individual workflows using AI, then a low-setup web browser experience is likely sufficient for our purposes. However, if we're looking to begin integrating AI into our products, customer experiences, or business processes, we'll need the flexibility of working with the API using a programming language.\n",
    "- **API**\n",
    "  - Provides *flexibility* for integrating AI into products, experiences, and processes\n",
    "  - Interact via programming language\n",
    "- **Web Interface**\n",
    "  - Very little setup\n",
    "  - Sufficient for *streamlining* individual's workflows\n",
    "\n",
    "7. Why the OpenAI API?: The beauty of making these models available via an API is that software engineers, developers, or anyone else wanting to integrate AI into products and services, can now access and implement these models without needing a background in data science or machine learning.\n",
    "- Accessibility to AI\n",
    "- Don't require data or ML experience\n",
    "\n",
    "8. Building AI applications: The OpenAI API has enabled the development of powerful new features, that before, would have required enormous computational resources and data. In the example shown from DataCamp workspace, we developed a feature where users can write or fix code by providing an instruction—all built on the OpenAI API. AI-powered products and services can provide much greater personalization to customers, so experiences can be tailored to an individual's needs and preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a65c825dddf8d66",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Applications built on the OpenAI API\n",
    "\n",
    "Software applications, web browser experiences, and even whole products are being built on top of the OpenAI API. In this exercise, you'll be able to explore an application built on top of the OpenAI API: DataCamp's own version of ChatGPT!\n",
    "\n",
    "The text you type into the interface will be sent as a request to the OpenAI API and the response will be delivered and unpacked directly back to you.\n",
    "\n",
    "Using the ChatGPT interface, answer the following question: In what year was OpenAI founded?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "- **2015**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b307452ae632385",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**There are countless other applications out there that harness the power of AI to solve complex problems and streamline the way you work.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a1ac19a23dc68",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Understanding the OpenAI space\n",
    "\n",
    "There's no better time to be learning about AI! Advancements in artificial intelligence has meant that AI-powered product features and tools are within reaching distance for more individuals and organizations than ever before.\n",
    "\n",
    "In this exercise, you'll test your understanding of some of the major players in the AI space: OpenAI, ChatGPT, and the OpenAI API.\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "**OpenAI**\n",
    "- Organization for researching and developing AI\n",
    "- Developed ChatGPT\n",
    "\n",
    "**ChatGPT**\n",
    "- Users have conversations using plain text\n",
    "- AI application for generating content\n",
    "\n",
    "**OpenAI API**\n",
    "- Users can decide which model to use and customize its behavior\n",
    "- Users create API requests to communicate model usage\n",
    "- Interface for accessing and customizing AI models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8647ac4a416a50d7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Now that you understand what the OpenAI API is and how powerful and accessible the functionality it provides is, head on over to the next video to learn how to leverage it with Python code!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf2b07575862235",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Making requests to the OpenAI API\n",
    "\n",
    "1. Making requests to the OpenAI API: Welcome back! In this video, you'll learn to make your very first request to the OpenAI API!\n",
    "\n",
    "2. APIs recap...: Recall that we can request the use of a model from OpenAI by making a request to their API.\n",
    "\n",
    "3. API endpoints: Depending on the model or services required, APIs have different access points for users. These access points are called endpoints. Endpoints are a lot like doors in a hospital. Depending on the treatment required, patients use different doors to reach different departments, and likewise, users can use different API endpoints to request different services.\n",
    "- **Endpoints** → API access point designed for specific interactions\n",
    "\n",
    "4. API authentication: Endpoints, like many hospital departments, may also require authentication before accessing services. API authentication is usually in the form of providing a unique key containing a random assortment of characters.\n",
    "- **Authentication** → Controls on access to API endpoint services (often unique key)\n",
    "\n",
    "5. API usage costs: It's important to note that many APIs, including the OpenAI API, have costs associated with using their services. For OpenAI, these costs are dependent on the model requested and on the size of the model input and output.\n",
    "- [OpenAI pricing][1]\n",
    "- For OpenAI API:\n",
    "  - Larger inputs + outputs = Greater cost\n",
    "\n",
    "6. API documentation: A crucial part of working with APIs is navigating API documentation, which provides details on which endpoints to use, their functionality, and how to set up authentication. Throughout the course, we'll provide these API details, but we also recommend checking out OpenAI's excellent API documentation.\n",
    "- [OpenAI API documentation][2]\n",
    "\n",
    "7. Creating an OpenAI API key: Usage of OpenAI's API requires authentication, so to use the API and complete the exercises in this course, you'll need to set up an account. OpenAI often provides free trial credit to new users, which will be more than sufficient for completing this course; however, for some countries, you may need to add a small amount of credit. Creating an OpenAI API key: Then, for authentication, you'll need to create a secret key and copy it. DataCamp doesn't store any API keys used in this course, so you can copy it directly into the exercises. Let's make our first API request!\n",
    "\n",
    "8. Making a request: There are several ways to interact with an API, but in this course, we'll use OpenAI's own Python library, which abstracts away a lot of the complexity of working with an API. We start by importing the OpenAI class from openai, which we'll use to instantiate a Python API client. The client configures the environment for communicating with the API. Within this function, we specify our API key, which is used to authenticate requests. Now for the API request code. We'll start by creating a request to the completions endpoint, which is used for completing a text prompt, by calling the create method on client.completions. Inside this method, we specify the model and the prompt to send to it. We'll discuss prompts in greater detail later in the course. Let's take a look at the API response.\n",
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"ENTER YOUR KEY HERE\")\n",
    "\n",
    "response = client.completions.create(model=\"gpt-3.5-turbo-instruct\", prompt=\"What is the OpenAI API?\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "9. The response: Here's the response. There's a lot of information in the output, so we'll add some spacing to make it more readable. The response from the API is a Completion object, which has various attributes for accessing the different information it contains. It has an id attribute, and choices, created, model, and other attributes below. We can see that the text response is located under the .choices attribute,\n",
    "```python\n",
    "Completion(id='cmpl-8S0D6VZBpM8Vy0fZf6atE71b1Zdvm',\n",
    "           choices=[CompletionChoice(finish_reason='length', index=0, logprobs=None, text='\\n\\nThe OpenAI API is an application programming interface provided by OpenAI, a')],\n",
    "           created=1701684684,\n",
    "           model='gpt-3.5-turbo-instruct',\n",
    "           object='text_completion',\n",
    "           system_fingerprint=None,\n",
    "           usage=CompletionUsage(completion_tokens=16, prompt_tokens=7, total_tokens=23))\n",
    "```\n",
    "\n",
    "10. Interpreting the response: so we'll start by accessing that. Attributes are accessed using a dot, then the name of the attribute. We've gotten much closer to the text. Notice from the square brackets at the beginning and end, that this is actually a list with a single element. Let's extract the first element to dig further. Ok - now we're left with a CompletionChoice object, which has its own set of attributes. Our text response is located underneath its .text attribute, which we can chain to our existing code. There we have it—our text response as a string! We started off with a complex object, but by taking it one attribute at a time, we were able to get to the result.\n",
    "```python\n",
    "print(response.choices)\n",
    "\n",
    "[out]:\n",
    "[CompletionChoice(finish_reason='length', index=0, logprobs=None, text='\\n\\nThe OpenAI API is an application programming interface provided by OpenAI, a')]\n",
    "```\n",
    "\n",
    "```python\n",
    "print(response.choices[0])\n",
    "\n",
    "[out]:\n",
    "CompletionChoice(finish_reason='length', index=0, logprobs=None, text='\\n\\nThe OpenAI API is an application programming interface provided by OpenAI, a')\n",
    "```\n",
    "\n",
    "```python\n",
    "print(response.choices[0].text)\n",
    "\n",
    "[out]:\n",
    "The OpenAI API is an application programming interface provided by OpenAI, a\n",
    "```\n",
    "\n",
    "11. Converting the response into a dictionary: In some cases, we may wish to work with a dictionary instead. We can convert the response into a dictionary with the .model_dump() method. In dictionary form, the response's attributes become dictionary keys, but the structure remains the same.\n",
    "```python\n",
    "print(response.model_dump())\n",
    "\n",
    "[out]:\n",
    "{'id': 'cmpl-8S0D6VZBpM8Vy0fZf6atE71b1Zdvm',\n",
    " 'choices': [{'finish_reason': 'length',\n",
    "   'index': 0,\n",
    "   'logprobs': None,\n",
    "   'text': '\\n\\nThe OpenAI API is an application programming interface provided by OpenAI, a'}],\n",
    " 'created': 1701684684,\n",
    " 'model': 'gpt-3.5-turbo-instruct',\n",
    " 'object': 'text_completion',\n",
    " 'system_fingerprint': None,\n",
    " 'usage': {'completion_tokens': 16, 'prompt_tokens': 7, 'total_tokens': 23}}\n",
    "```\n",
    "\n",
    "[1]: https://openai.com/pricing\n",
    "[2]: https://platform.openai.com/docs/api-reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3881239d-f674-4552-a80a-f70bc75b635b",
   "metadata": {},
   "source": [
    "### Your first API request!\n",
    "\n",
    "Throughout the course, you'll write Python code to interact with the OpenAI API. As a first step, you'll need to create your own API key. **API keys used in this course's exercises will not be stored in any way**.\n",
    "\n",
    "To create a key, you'll first need to create an OpenAI account by visiting their [signup page][1]. Next, navigate to the [API keys page][2] to create your secret key.\n",
    "\n",
    "The button to create a new secret key.\n",
    "\n",
    "OpenAI sometimes provides free credits for the API, but this can differ depending on geography. You may also need to add debit/credit card details. **You'll need less than $1 credit to complete this course**.\n",
    "\n",
    "**Warning**: if you send many requests or use lots of tokens in a short period, you may see an openai.error.RateLimitError. If you see this error, please wait a minute for your quota to reset and you should be able to begin sending more requests. Please see [OpenAI's rate limit error support article][3] for more information.\n",
    "\n",
    "[1]: https://platform.openai.com/signup\n",
    "[2]: https://platform.openai.com/account/api-keys\n",
    "[3]: https://help.openai.com/en/articles/6897202-ratelimiterror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37792f0b1c2e1740",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# https://platform.openai.com/account/billing/overview\n",
    "\n",
    "# read the API key from a config.ini\n",
    "config = ConfigParser()\n",
    "\n",
    "config.read(r'D:\\\\users\\\\trenton\\\\Dropbox\\\\PythonProjects\\\\config_api.ini')\n",
    "api_key = config['openai']['key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f46e6-d815-4f07-a95a-62b0c8391fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the OpenAI class\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c4c3fa893fce5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# call the completion endpoint\n",
    "response = client.completions.create(model='gpt-3.5-turbo-instruct', prompt='Who developed ChatGPT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a1fe3939a5b5b9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# print the response\n",
    "print(response.choices[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc6cfdea0ec9fd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1683830aad95d1d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**You've just taken your very first steps on the road to creating awesome AI-powered applications and experiences. In the next exercise, you'll practice digging into the JSON response to extract the returned text.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de3d36749916a1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Digging into the response\n",
    "\n",
    "One of the key skills required to work with APIs is manipulating the response to extract the desired information. In this exercise, you'll push your Python dictionary and list manipulation skills to the max to extract information from the API response.\n",
    "\n",
    "You've been provided with `response`, which is a response from the OpenAI API when provided with the prompt, What is the goal of OpenAI?\n",
    "\n",
    "This `response` object has been printed for you so you can see and understand its structure. If you're struggling to picture the structure, view the dictionary form of the response with `.model_dump()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb1264fc66860d9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2262466b069b1190",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "response.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1071dfb57dddac05",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "response.usage.total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637a353a17c7db9d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "response.choices[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95b2a1b6dae882",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Throughout the course, you'll learn to perform lots of different tasks by making requests to the OpenAI API, and each time, you'll use similar subsetting to dig into the response and extract the result.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bc4f3839f310fc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## The OpenAI developer ecosystem\n",
    "\n",
    "1. The OpenAI landscape: Despite being renowned for its GPT series of chat models, OpenAI hosts a diverse array of models capable of performing many different tasks. In this course, we'll be focusing primarily on OpenAI's text-based models, but later in the course, we'll also take a look at the audio transcription and translation capabilities of the Whisper model. For now, however, let's take a closer look at the text capabilities available through the API.\n",
    "- Images\n",
    "- **Chat**\n",
    "- Embeddings\n",
    "- Audio\n",
    "- **Completion**\n",
    "- Fine-tuning\n",
    "- **Moderation**\n",
    "\n",
    "2. Completions: The Completions endpoint allows users to send a prompt and receive a model-generated response that attempts to complete the prompt in a likely and consistent way. Completions is used for so-called single-turn tasks, as there is a single prompt and response. However, the models available via this endpoint are extremely flexible, and are capable of answering questions, performing classification tasks, determining text sentiment, explaining complex topics, and much more.\n",
    "- Receive continuation of a prompt\n",
    "- Single-turn tasks\n",
    "  - Answer questions\n",
    "  - Classification into categories\n",
    "  - Sentiment analysis\n",
    "  - Explain complex topics\n",
    "\n",
    "3. Chat: The Chat endpoint can be used for applications that require multi-turn tasks, including assisting with ideation, customer support questions, personalized tutoring, translating languages, and writing code. Chat models also perform well on single-turn tasks, so many applications are built on top of chat models for flexibility. We'll cover how to use Chat later in the course.\n",
    "- Multi-turn conversations\n",
    "  - Ideation\n",
    "  - Customer support assistant\n",
    "  - Personal tutor\n",
    "  - Translate languages\n",
    "  - Write code\n",
    "- Also performs well on single-turn tasks\n",
    "- Chat completions → Chapter 2\n",
    "\n",
    "4. Moderation: The Moderation endpoint is used to check whether content violates OpenAI's usage policies, such inciting violence or promoting hate speech. The sensitivity of the model to different types of violations can be customized for specific use cases that may require stricter or more lenient moderation.\n",
    "- [Usage policies][1]\n",
    "- [Overview][2]\n",
    "- Check content for violations of OpenAI's usage policies, including:\n",
    "  - Inciting violence\n",
    "  - Hate speech\n",
    "- Can customize model sensitivity to specific violations\n",
    "\n",
    "5. Organizations: For business use cases with frequent requests to the API, it's important to manage usage across the business. Setting up an organization for the API allows for better management of access, billing, and usage limits to the API. Users can be part of multiple organizations and attribute requests to specific organizations for billing. To attribute a request to a specific organization, we only need to add one more line of code. Like the API key, the organization ID can be set before the request.\n",
    "- [Organization Setting][3]\n",
    "```python\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"ENTER YOUR KEY HERE\", organization = \"ENTER ORG ID\")\n",
    "response = client.completions.create(model=\"gpt-3.5-turbo-instruct\", prompt=\"What is the OpenAI API?\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "6. Rate limits: API rate limits are another key consideration for companies building features on the OpenAI API. Rate limits are a cap on the frequency and size of API requests. They are put in place to ensure fair access to the API, prevent misuse, and also manage the infrastructure that supports the API. For many cases, this may not be an issue, but if a feature is exposed to a large user base, or the requests require generating large bodies of content, they could be at risk of hitting the rate limits.\n",
    "- [Rate Limits][4]\n",
    "- Cap on frequency and size of API requests\n",
    "\n",
    "7. Organization structure: Much of this risk can be mitigated by, instead of running multiple features under the same organization, having separate organizations for each business unit or product feature, depending on the number of features built on the OpenAI API. In this example, we've created separate OpenAI organizations for three different AI-powered features: a customer service chatbot, a content recommendation system, and a video transcript generator. This distributes the requests to reduce the risk of hitting the rate limit. It also removes the single failure point, so an issue to one organization, such as a billing issue, will only result in the failure of a single feature. Product-separated organizations also provide more granular insights into usage and billing.\n",
    "- ![][5]\n",
    "- Distributes requests across organizations\n",
    "  - Reduced likelihood of hitting rate limits\n",
    "- Removes single failure point\n",
    "  - Issue with one organization doesn't break all features\n",
    "- Better usage and billing management\n",
    "\n",
    "[1]: https://openai.com/policies/usage-policies\n",
    "[2]: https://platform.openai.com/docs/guides/moderation/overview\n",
    "[3]: https://platform.openai.com/account/org-settings\n",
    "[4]: https://platform.openai.com/docs/guides/rate-limits\n",
    "[5]: https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/2024-01-29_working_with_the_openai_api/oai04.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dadf558d198c6e3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Solving problems with AI solutions\n",
    "\n",
    "An Online Scientific Journal called Terra Scientia wants to use AI to make their scientific papers more accessible to a wider audience. To do this, they want to develop a feature where users can double-click on words they don't understand, and an AI model will explain what it means in the context of the article.\n",
    "\n",
    "To accomplish this, the developers at Terra Scientia want to build the feature on top of the OpenAI API.\n",
    "\n",
    "Which OpenAI API endpoint(s) could they use to build this feature?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "- **Chat**\n",
    "- **Moderation**\n",
    "\n",
    "**Evaluating the situation and model capabilities to see if (1) AI can be used to solve the problem, and (2) what the implementation would look like is the crucial starting place for AI feature development.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6f0133dbedee6a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Structuring organizations\n",
    "\n",
    "You've learned that you can set up organizations to manage API usage and billing. Users can be part of multiple organizations and attribute API requests to a specific organization. It's best practice to structure organizations such that each business unit or product feature has a separate organization, depending on the number of features the business has built on the OpenAI API.\n",
    "\n",
    "What are the benefits of having separate organizations for each business unit or product feature?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "- **Reducing risk of hitting rate limits**\n",
    "- **Improving insights into usage and billing**\n",
    "- **Removing single failure points**\n",
    "\n",
    "**Managing organizations will ensure that users can have an interruption-free experience that translates the power of AI into real business value.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949a4f7d931bfce2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# OpenAI's Text and Chat Capabilities\n",
    "\n",
    "OpenAI's GPT series of language models have created headlines the world over. In this chapter, you'll use these models for generating and transforming text content, for classification tasks like categorization and sentiment analysis, and finally, to create your very own AI-powered chatbot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06394981a5c3446",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Text completions\n",
    "\n",
    "1. Text completions: In this chapter, we'll take a deeper dive into the rich text and chat functionality the API has to offer.\n",
    "\n",
    "2. Recap...: So far, we've used the Completions endpoint to answer questions, but the model's capabilities go far beyond this. To understand where these capabilities come from, let's take a step back and discuss how text completion works.\n",
    "- Text completions for Q&A\n",
    "```python\n",
    "response = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=\"How many days are in October?\"\n",
    ")\n",
    "print(response.choices[0].text)\n",
    "[out]:\n",
    "October has 31 days.\n",
    "```\n",
    "\n",
    "3. What is a text completion?: When we send a prompt to the Completions endpoint, the model returns the text that it believes is most likely to complete the prompt, which it infers based on the data the model was developed on. If we send \"Life is like a box of chocolates\" to the model, it correctly completes the quote with high probability. We say high probability here because the model results are non-deterministic, so the model may only correctly complete the quote 98 times out of 100.\n",
    "- Text most likely to complete the prompt\n",
    "```python\n",
    "response = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=\"Life is like a box of chocolates.\"\n",
    ")\n",
    "print(response.choices[0].text)\n",
    "[out]:\n",
    "You never know what you're going to get.\n",
    "```\n",
    "- Response is non-deterministic (inherently random)\n",
    "\n",
    "4. Controlling response randomness: There are many use cases where randomness is undesirable; think of a customer service chatbot - we wouldn't want the chatbot to provide different guidance to customers with the same issue. However, we would like the model to be flexible to different inputs, so there's often a trade off in the amount of randomness. We can control the amount of randomness in the response using the temperature parameter. temperature is set to one by default, but can range from zero to two, where zero is almost entirely deterministic and two is extremely random. If we add a temperature of two here, we can see the model completes the prompt by putting its own bizarre spin on Forrest Gump's famous quote.\n",
    "- `temperature` : control on determinism\n",
    "- Ranges from 0 (highly deterministic) to 2 (very random)\n",
    "```python\n",
    "response = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=\"Life is like a box of chocolates.\",\n",
    "    temperature=2\n",
    ")\n",
    "print(response.choices[0].text)\n",
    "[out]:\n",
    "You never know what bitter constraints to any fate journey of Enlightenment you instead be forced...\n",
    "```\n",
    "\n",
    "5. Content transformation: Because the text completion model returns the most likely text to follow the prompt, it can be used to solve a number of tasks besides answering questions, including text content generation and transformation. Text transformation involves changing text based on an instruction, and examples include find and replace, summarization, and copyediting. For example, we can use the API to update the name, pronouns, and job title in a bio. Notice that the prompt starts with the instruction, then the text to transform. We've also used triple quotes to define a multi-line prompt for ease of readability and processing. Then, as before, we send this prompt to the Completions endpoint of the API. Voilà! We have our updated text. Even with a find and replace tool, this task would normally require us to specify every word to update.\n",
    "- Changing text based on an instruction\n",
    "  - Find and replace\n",
    "  - Summarization\n",
    "  - Copyediting\n",
    "```python\n",
    "prompt = \"\"\"Update name to Maarten, pronouns to he/him, and job title to Senior Content Developer: Joanne is a Content Developer at DataCamp. Her favorite programming language is R, which she uses for her statistical analyses.\"\"\"\n",
    "\n",
    "response = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=prompt\n",
    ")\n",
    "print(response.choices[0].text)\n",
    "[out]:\n",
    "Maarten is a Senior Content Developer at DataCamp. His favorite programming language is R, which he uses for his statistical analyses.\n",
    "```\n",
    "\n",
    "6. Content generation: Text completions are also used to generate new text content from a prompt providing an instruction. For example, we can create a request to generate a tagline for a new hot dog stand - the API does a good job, and even includes a subtle pun!\n",
    "```python\n",
    "response = client.completions.create(\n",
    "model=\"gpt-3.5-turbo-instruct\",\n",
    "prompt=\"Create a tagline for a new hot dog stand.\"\n",
    ")\n",
    "print(response.choices[0].text)\n",
    "[out]:\n",
    "\"Frankly, we've got the BEST dogs in town!\"\n",
    "```\n",
    "\n",
    "7. Controlling response length: By default, the response from the API is quite short, which may be unsuitable for many use cases. The max_tokens parameter can be used to control the maximum length of the response.\n",
    "- Default `max_tokens`\n",
    "```python\n",
    "response = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=\"Write a haiku about AI.\"\n",
    ")\n",
    "print(response.choices[0].text)\n",
    "\n",
    "[out]:\n",
    "AI so powerful\n",
    "Computers that think and learn\n",
    "Superseding\n",
    "```\n",
    "- `max_tokens=30`\n",
    "```python\n",
    "response = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=\"Write a haiku about AI.\",\n",
    "    max_tokens=30\n",
    ")\n",
    "print(response.choices[0].text)\n",
    "\n",
    "[out]:\n",
    "A machine mind thinks\n",
    "Logic dictates its choices\n",
    "Mankind ponders anew\n",
    "```\n",
    "\n",
    "8. Understanding tokens: Tokens are a unit of one or more characters used by language models to understand and interpret text. In English, one token translates to about four characters, and 100 tokens to 75 words, so if our use case requires no more than around 150 words, a max_tokens of 200 would be a good choice.\n",
    "- ![][1]\n",
    "- In English:\n",
    "  - 1 token ~ 4 characters\n",
    "  - 100 tokens ~ 75 words\n",
    "  - Example: 150 words → `max_tokens=200`\n",
    "\n",
    "9. Returning to cost: Increasing max_tokens will likely also increase the usage cost for each request. Recall that the usage costs are dependent on the model used and the amount of generated text. Each model is actually priced based upon the cost per 1000 tokens, where input tokens, the tokens used in the prompt, and output tokens, the generated text, can be priced differently. When scoping the potential cost of a new AI feature, the first step is often a back-of-the-envelope calculation to determine the cost per unit time.\n",
    "- Increasing max_tokens increases cost\n",
    "- Usage costs dependent on amount of generated text\n",
    "  - Models are priced by cost/1K tokens\n",
    "  - Input and output tokens can be priced differently\n",
    "- Scoping feature cost often starts with a rough calculation:\n",
    "- ![][2]\n",
    "- [OpenAI pricing][3]\n",
    "\n",
    "[1]: https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/2024-01-29_working_with_the_openai_api/oai01.jpg\n",
    "[2]: https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/2024-01-29_working_with_the_openai_api/oai02.jpg\n",
    "[3]: https://openai.com/pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e74719d34aa9b2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Find and replace\n",
    "\n",
    "Text completion models can be used for much more than answering questions. In this exercise, you'll explore the model's ability to transform a text prompt.\n",
    "\n",
    "Find-and-replace tools have been around for decades, but they are often limited to identifying and replacing exact words or phrases. You've been provided with a block of text discussing cars, and you'll use a completion model to update the text to discuss planes instead, updating the text appropriately.\n",
    "\n",
    "Warning**: if you send many requests or use lots of tokens in a short period, you may hit your rate limit and see an `openai.error.RateLimitError`. If you see this error, please wait a minute for your quota to reset and you should be able to begin sending more requests. Please see [OpenAI's rate limit error support article][1] for more information.\n",
    "\n",
    "[1]: https://help.openai.com/en/articles/6897202-ratelimiterror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbfcff34ad60e43",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set your API key\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "prompt = \"\"\"Replace car with plane and adjust phrase:\n",
    "A car is a vehicle that is typically powered by an internal combustion engine or an electric motor. It has four wheels, and is designed to carry passengers and/or cargo on roads or highways. Cars have become a ubiquitous part of modern society, and are used for a wide variety of purposes, such as commuting, travel, and transportation of goods. Cars are often associated with freedom, independence, and mobility.\"\"\"\n",
    "\n",
    "# Create a request to the Completions endpoin\n",
    "response = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=prompt,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# Extract and print the response text\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3561d78f23a3bd82",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Quickly adapting text in this way was impossible before AI came along, and now, anyone can do this with just a few lines of code and an OpenAI API key. Head on over to the next exercise to try another transformation task.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf8bf0c46d4888",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Text summarization\n",
    "\n",
    "One really common use case for using OpenAI's models is summarizing text. This has a ton of applications in business settings, including summarizing reports into concise one-pagers or a handful of bullet points, or extracting the next steps and timelines for different stakeholders.\n",
    "\n",
    "In this exercise, you'll summarize a passage of text on financial investment into two concise bullet points using a text completion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206d6a924b5f83f8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Summarize the following text into two concise bullet points:\n",
    "Investment refers to the act of committing money or capital to an enterprise with the expectation of obtaining an added income or profit in return. There are a variety of investment options available, including stocks, bonds, mutual funds, real estate, precious metals, and currencies. Making an investment decision requires careful analysis, assessment of risk, and evaluation of potential rewards. Good investments have the ability to produce high returns over the long term while minimizing risk. Diversification of investment portfolios reduces risk exposure. Investment can be a valuable tool for building wealth, generating income, and achieving financial security. It is important to be diligent and informed when investing to avoid losses.\"\"\"\n",
    "\n",
    "# Create a request to the Completions endpoint\n",
    "response = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=prompt,\n",
    "    max_tokens=400,\n",
    "    temperature=0)\n",
    "\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff7c492ced571c7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Summarization is one of the most widely-used capabilities of OpenAI's completions models. This has many applications in business to condense and personalize reports and papers to a particular length and audience.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28fffc103c222f9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Content generation\n",
    "\n",
    "AI is playing a much greater role in content generation, from creating marketing content such as blog post titles to creating outreach email templates for sales teams.\n",
    "\n",
    "In this exercise, you'll harness AI through the Completions endpoint to generate a catchy slogan for a new restaurant. Feel free to test out different prompts, such as varying the type of cuisine (e.g., Italian, Chinese) or the type of restaurant (e.g., fine-dining, fast-food), to see how the response changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc21ab4b35b7590e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a request to the Completions endpoint\n",
    "response = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt='create a slogan for a new restaurant',\n",
    "    max_tokens=100)\n",
    "\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd14c3f4b65c380",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**AI-generated content, including text, images, audio, and even video, is becoming more and more mainstream, particularly for industries and roles that require creating user-facing content, such as media and marketing. Head on over to the next video to learn about more unexpected and impressive tasks that you can complete with OpenAI's models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ecd2aa577ea824",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Text completions for classification tasks\n",
    "\n",
    "1. Text completions for classification tasks: So we've seen that the text completions models are pretty flexible, and can do much more than answer questions. In this lesson, we'll take this a step further to discuss classification tasks.\n",
    "\n",
    "2. Classification tasks: Classification tasks involve assigning a label to a piece of information. This can be identification, such as identifying the language used in a piece of text, categorization, such as sorting geographical locations into countries and US states, or even classifying a statement's sentiment, that is, whether it sounds positive or negative. The Completions endpoint can be used for all of these tasks, providing the model has sufficient knowledge of the subject area and that the prompt contains the necessary context.\n",
    "- ![][1]\n",
    "- Task that involves assigning a label to information\n",
    "  - Identifying the language from text\n",
    "  - Categorization\n",
    "  - Classifying sentiment\n",
    "- Completions endpoint can perform these tasks, providing:\n",
    "  - Model has sufficient knowledge\n",
    "  - Prompt contains sufficient context\n",
    "\n",
    "3. Categorizing animals: Let's use the Completions endpoint to categorize these animals. Printing the response, we see that the model categorized the animals into mammals, fish, and reptiles. This might be what we were looking for, but there's an almost infinite number of ways to categorize something, so it's best practice to state the desired categories in the prompt.\n",
    "```python\n",
    "response = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=\"Classify the following animals into categories: zebra, crocodile, blue whale, polar bear, salmon, dog.\",\n",
    "    max_tokens = 50\n",
    ")\n",
    "print(response.choices[0].text)\n",
    "\n",
    "[out]:\n",
    "Mammals: Zebra, Polar Bear, Dog\n",
    "Fish: Salmon\n",
    "Reptiles: Crocodile\n",
    "```\n",
    "\n",
    "4. Specifying groups: We can update the prompt to categorize animals into those with and without fur, and the model responds with the desired categories.\n",
    "```python\n",
    "response = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=\"Classify the following animals into animals with fur and without: zebra, crocodile, dolphin, polar bear, salmon, dog.\",\n",
    "    max_tokens=50\n",
    ")\n",
    "print(response.choices[0].text)\n",
    "\n",
    "[out]:\n",
    "Animals with fur: Dog, Polar Bear, Zebra\n",
    "Animals without fur: Crocodile, Dolphin, Salmon\n",
    "```\n",
    "\n",
    "5. Classifying sentiment: Let's look at an example of classifying sentiment. Let's say we received a number of online reviews to our restaurant, and we want to extract the sentiment, as numbers one to five, for further analysis. We can write a prompt with the instruction to classify sentiment, and a list of statements, pass it to the Completions endpoint, and print the response. The model does a good job; however, because we didn't specify how to classify the categories, the model didn't realize we actually wanted a numerical output. We can update our prompt so the model has the context on what the categories should be. The updated response contains a reasonable numerical representation of the sentiment of each statement.\n",
    "```python\n",
    "prompt = \"\"\"Classify sentiment in the following statements:\n",
    "1. The service was very slow\n",
    "2. The steak was awfully tasty!\n",
    "3. Meal was decent, but I've had better.\n",
    "4. My food was delayed, but drinks were good.\n",
    "\"\"\"\n",
    "response = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=prompt,\n",
    "    max_tokens=50\n",
    ")\n",
    "print(response.choices[0].text)\n",
    "```\n",
    "- The service was very slow: **Negative**\n",
    "- The steak was awfully tasty!: **Positive**\n",
    "- Meal was decent, but I've had better.: **Neutral**\n",
    "- My food was delayed, but drinks were good.: **Mixed**\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"Classify sentiment as 1-5 (bad-good) in the following statements:\n",
    "1. The service was very slow\n",
    "2. The steak was awfully tasty!\n",
    "3. Meal was decent, but I've had better.\n",
    "4. My food was delayed, but drinks were good.\n",
    "\"\"\"\n",
    "\n",
    "[out]:\n",
    "1. The service was very slow: 1\n",
    "2. The steak was awfully tasty!: 5\n",
    "3. Meal was decent, but I've had better.: 3\n",
    "4. My food was delayed, but drinks were good.: 2\n",
    "```\n",
    "\n",
    "6. Zero-shot vs. one-shot vs. few-shot prompting: The prompts we've used so far are examples of a so-called zero-shot prompts, where we haven't provided any examples in the prompt for the model to learn from. For many use cases, this may be fine, but for more complex cases where the model may not have a good understanding of the subject matter or task, we may need to provide examples for it to learn from. If one example is provided in the prompt, it's called one-shot prompting, and for more than one, it's called few-shot prompting. Providing examples in the prompt for the model to learn from is called in-context learning, as the model is learning from the context we're providing. Let's return to sentiment analysis to give these prompting techniques a go.\n",
    "- Zero-shot prompting: no examples provided\n",
    "- In-context learning:\n",
    "  - One-shot prompting: one example provided\n",
    "  - Few-shot prompting: a handful of examples provided\n",
    "\n",
    "7. One-shot prompting: We're again dealing with restaurant reviews, but this time, we want the sentiment to be more specific. To do this, we provide one example in the prompt, clearly separating the input and output. Sending this to the Completions endpoint and printing the result shows that the model accurately analyzed the statement, but that it didn't provide any more detail than when we used a zero-shot prompt. In more complex cases, one example is not enough, and we'll need to experiment to determine the number of shots the model needs for the use case.\n",
    "```python\n",
    "prompt = \"\"\"Classify sentiment in the following statements:\n",
    "The service was very slow // Disgruntled\n",
    "Meal was decent, but I've had better. //\n",
    "\"\"\"\n",
    "response = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=prompt\n",
    ")\n",
    "print(response.choices[0].text)\n",
    "\n",
    "[out]:\n",
    "Neutral\n",
    "```\n",
    "\n",
    "8. Few-shot prompting: Let's extend the prompt to provide three examples. Running the same code again, we see that the model was much more specific.\n",
    "```python\n",
    "prompt = \"\"\"Classify sentiment in the following statements:\n",
    "The service was very slow // Disgruntled\n",
    "The steak was awfully tasty! // Delighted\n",
    "Good experience overall. // Satisfied\n",
    "Meal was decent, but I've had better. //\n",
    "\"\"\"\n",
    "response = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=prompt\n",
    ")\n",
    "print(response.choices[0].text)\n",
    "\n",
    "[out]:\n",
    "Mildly dissatisfied\n",
    "```\n",
    "\n",
    "9. Let's practice!: Hopefully you're starting to grasp just how many problems can be solved using the OpenAI API, and how important prompts are to obtaining good results. Time to practice!\n",
    "\n",
    "\n",
    "[1]: https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/2024-01-29_working_with_the_openai_api/oai03.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b278a8ae7f2d0ce5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Classifying text sentiment\n",
    "\n",
    "As well as answering questions, transforming text, and generating new text, Completions models can also be used for classification tasks, such as categorization and sentiment classification. This sort of task requires not only knowledge of the words but also a deeper understanding of their meaning.\n",
    "\n",
    "In this exercise, you'll explore using Completions models for sentiment classification using reviews from an online shoe store called Toe-Tally Comfortable:\n",
    "\n",
    "- Unbelievably good!\n",
    "- Shoes fell apart on the second use.\n",
    "- The shoes look nice, but they aren't very comfortable.\n",
    "- Can't wait to show them off!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5298e3fa7272add",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a request to the Completions endpoint\n",
    "# Create a request to the Completions endpoint\n",
    "response = client.completions.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\",\n",
    "  prompt=\"\"\"classify sentiment as negative, positive, or neutral in the following statements:\n",
    "1. Unbelievably good!\n",
    "2. Shoes fell apart on the second use.\n",
    "3. The shoes look nice, but they aren't very comfortable.\n",
    "4. Can't wait to show them off!\n",
    "\"\"\",\n",
    "  max_tokens=100\n",
    ")\n",
    "\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944bf88845f013d3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Categorizing companies\n",
    "\n",
    "In this exercise, you'll use a Completions model to categorize different companies. At first, you won't specify the categories to see how the model categorizes them. Then, you'll specify the categories in the prompt to ensure they are categorized in a desirable and predictable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8433d36bba2b82",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Categorize the following companies: Apple, Microsoft, Saudi Aramco, Alphabet, Amazon, Berkshire Hathaway, NVIDIA, Meta, Tesla, and LVMH\",\n",
    "    \"Categorize the following companies into the categories Tech, Energy, Luxury Goods, or Investment: Apple, Microsoft, Saudi Aramco, Alphabet, Amazon, Berkshire Hathaway, NVIDIA, Meta, Tesla, and LVMH\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    # Create a request to the Completions endpoint\n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=100,\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    print(response.choices[0].text)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8efa1689062e6c5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Categorically commendable! Providing a more specific prompt gave you much greater control over the model's response. In the next video, you'll learn about OpenAI's chat models, which are the engines behind popular AI chatbot applications like ChatGPT. You can do this!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abdcf4382942101",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Chat completions with GPT\n",
    "\n",
    "1. Chat completions with GPT: Welcome back! In this video, we'll learn to unlock the chat capabilities from the OpenAI API, which underpin popular applications like ChatGPT. Let's get started!\n",
    "\n",
    "2. The Chat Completions endpoint: The Chat Completions endpoint allows us to have multi-turn conversations with a model, so we can build on our previous prompts depending on how the model responds. Additionally, chat models often perform just as well as Completions models on single-turn tasks. Compared to the Completions endpoint, Chat Completion allows for better customizability of the response through the use of roles, which we'll discuss in a moment. Finally, there's also a cost benefit to using chat completions models over completions. The cost benefit and flexibility of being able to have multi-turn conversations, means that developers quite often choose a chat model when building applications on the OpenAI API.\n",
    "- Multi-turn conversations\n",
    "  - Also performs well on single-turn\n",
    "- Better customizability of response through the use of roles\n",
    "- Cost benefit: gpt-3.5-turbo is cheaper than gpt-3.5-turbo-instruct\n",
    "\n",
    "3. Roles: Roles are at the heart of how chat models function. There are three main roles: the system, the user, and the assistant. The system role allows the user to specify a message to control the behavior of the assistant. For example, for a customer service chatbot, we could provide a system message stating that the assistant is a polite and helpful customer service assistant. The user provides an instruction to the assistant, and the assistant responds. One of the interesting things about chat models, is that the user can also provide assistant messages. These are often utilized to provide examples to help the model better understand the user's desired response. We'll discuss multi-turn conversations in the next video; for now, we'll get familiar with using chat models for single-turn tasks.\n",
    "- **System**: controls assistant's behavior\n",
    "- **User**: instruct the assistant\n",
    "- **Assistant**: response to user instruction\n",
    "  - Can also be written by the user to provide examples\n",
    "\n",
    "4. Request setup: Making a request to the Chat Completions endpoint is very similar to the Completions endpoint. Instead of calling the create method on client.completions, we call it on client.chat.completions; there's also different models for these two endpoints. The main difference is in the way that prompts are provided. For the Completions endpoint, the prompt is passed as a string for the model to complete. Due to the greater customizability of chat models through the use of roles, the prompt is provided in a different way. \n",
    "- Completions\n",
    "```python\n",
    "response = client.completions.create(model=\"gpt-3.5-turbo-instruct\", prompt=\"____\")\n",
    "```\n",
    "\n",
    "- Chat Completions\n",
    "```python\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=____)\n",
    "```\n",
    "\n",
    "5. Prompt setup: The prompt is set up by creating a list of dictionaries, where each dictionary provides content to one of the roles. The messages often start with the system role followed by alternating user and assistant messages. The system role here instructs the assistant to act as a data science tutor that speaks concisely.\n",
    "```python\n",
    "messages=[{\"role\": \"system\", \"content\": \"You are a data science tutor who speaks concisely.\"},\n",
    "          {\"role\": \"user\", \"content\": \"What is the difference between mutable and immutable objects?\"}]\n",
    "```\n",
    "6. Making a request: Let's add these messages into the request code and print the response.\n",
    "```python\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
    "                                          messages=[{\"role\": \"system\",\n",
    "                                                     \"content\": \"You are a data science tutor who speaks concisely.\"},\n",
    "                                                    {\"role\": \"user\",\n",
    "                                                     \"content\": \"What is the difference between mutable and immutable objects?\"}]\n",
    "                                          )\n",
    "print(response)\n",
    "```\n",
    "\n",
    "7. The response: The response we receive is very similar to the Completions endpoint, where the assistant's text response is nested inside the choices attribute. We'll start by accessing the\n",
    "```python\n",
    "ChatCompletion(id='chatcmpl-8SM9TkxEkunyu6gLVJv3OvSmnxXvX',\n",
    "               choices=[Choice(finish_reason='stop',\n",
    "                               index=0,\n",
    "                               message=ChatCompletionMessage(content='Mutable objects can be modified after they are created, whereas immutable objects cannot be modified once they are created.',\n",
    "                               role='assistant', function_call=None, tool_calls=None))],\n",
    "                created = 1701769027,\n",
    "                model = 'gpt-3.5-turbo-0613',\n",
    "                object = 'chat.completion',\n",
    "                system_fingerprint = None,\n",
    "                usage = CompletionUsage(completion_tokens=21, prompt_tokens=33, total_tokens=54))\n",
    "```\n",
    "\n",
    "8. Extracting the text: choices attribute, which returns a list with a single element, a Choice object with its own attributes. Next, we'll subset the first element from that list to get to the Choice object. Finally, we can access the content by first accessing the message attribute of the Choice object, and then the content attribute of the ChatCompletionMessage object. We can see that the assistant stayed true to the system message - only using a single sentence in its concise explanation.\n",
    "```python\n",
    "print(response.choices)\n",
    "\n",
    "[out]: \n",
    "[Choice(finish_reason='stop',\n",
    "        index=0,\n",
    "        message=ChatCompletionMessage(\n",
    "            content='Mutable objects can be modified after they are created, whereas immutable objects cannot be modified once they are created.',\n",
    "            role='assistant', function_call=None, tool_calls=None))]\n",
    "```\n",
    "\n",
    "```python\n",
    "print(response.choices[0])\n",
    "\n",
    "[out]: \n",
    "Choice(finish_reason='stop',\n",
    "       index=0,\n",
    "       message=ChatCompletionMessage(\n",
    "           content='Mutable objects can be modified after they are created, whereas immutable objects cannot be modified once they are created.',\n",
    "           role='assistant', function_call=None, tool_calls=None))\n",
    "```\n",
    "\n",
    "```python\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "[out]: \n",
    "Mutable objects can be modified after they are created, whereas immutable objects cannot be modified once they are created.\n",
    "```\n",
    "\n",
    "9. Let's practice!: In the next video, you'll learn how to extend this to multi-turn tasks, but for now, time for some practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d864deaafadc30ba",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### The Chat Completions endpoint\n",
    "\n",
    "The models available via the Chat Completions endpoint can not only perform similar single-turn tasks as models from the Completions endpoint, but can also be used to have multi-turn conversations.\n",
    "\n",
    "To enable multi-turn conversations, the endpoint supports three different roles:\n",
    "\n",
    "- **System**: controls assistant's behavior\n",
    "- **User**: instruct the assistant\n",
    "- **Assistant**: response to user instruction\n",
    "\n",
    "In this exercise, you'll make your first request to the Chat Completions endpoint to answer the following question:\n",
    "\n",
    "What is the difference between a for loop and a while loop?\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "- Create a request to the Chat Completions endpoint using both system and user messages to answer the question, _What is the difference between a for loop and a while loop?_\n",
    "- Extract and print the assistant's text response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e214fd965ed2af20",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a request to the Chat Completions endpoint\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_tokens=150,\n",
    "    messages=[\n",
    "        {\"role\": 'system',\n",
    "         \"content\": \"You are a helpful data science tutor.\"},\n",
    "        {'role': 'user',\n",
    "         'content': \"What is the difference between a for loop and a while loop?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract and print the assistant's text response\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c0bf1078dccc6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Exemplary explanation! AI models have massively augmented how people approach upskilling and learning; in the next exercise, you'll take this up a level to use the model for code explanation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ee36282267d3de",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Code explanation\n",
    "\n",
    "One of the most popular use cases for using OpenAI models is for explaining complex content, such as technical jargon and code. This is a task that data practitioners, software engineers, and many others must tackle in their day-to-day as they review and utilize code written by others.\n",
    "\n",
    "In this exercise, you'll use the OpenAI API to explain a block of Python code to understand what it is doing.\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "- Create a request to the Chat Completions endpoint to send `instruction` to the `gpt-3.5-turbo model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6defb9277c298510",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "instruction = \"\"\"Explain what this Python code does in one sentence:\n",
    "import numpy as np\n",
    "\n",
    "heights_dict = {\"Mark\": 1.76, \"Steve\": 1.88, \"Adnan\": 1.73}\n",
    "heights = heights_dict.values()\n",
    "print(np.mean(heights))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Create a request to the Chat Completions endpoint to send instruction to the gpt-3.5-turbo model.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a helpful Python programming tutor.\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": instruction}\n",
    "    ],\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d550c58436a41455",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Awesome work! Think of how useful this is for reviewing other people's code; it would even be possible to construct a prompt to write documentation or code comments for some code using this model. Continue on to discover how chat completions can be used to start building chatbots on the chat completions models!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4765b22b62868b2e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Multi-turn chat completions with GPT\n",
    "\n",
    "1. Multi-turn chat completions with GPT: Great work so far! In this video, we'll begin to unleash the full potential of chat models by creating multi-turn conversations.\n",
    "\n",
    "2. Chat completions for single-turn tasks: Recall that messages are sent to the Chat Completions endpoint as a list of dictionaries, where each dictionary provides content to a specific role from either system, user, or assistant. For single turn tasks, no content is sent to the assistant role - the model relies only on its existing knowledge, the behaviors sent to the system role, and the instruction from the user.\n",
    "\n",
    "3. Providing examples: Providing assistant messages can be a simple way for developers to steer the model in the right direction without having to surface anything to end-users. If we created a data science tutor application, we could provide a few examples of data science questions and answers that would be sent to the API along with the user's question. Let's improve our data science tutor by providing an example. Between the system message outlining the assistant's behavior and the user's question to answer, we'll add a user and assistant message to serve as an ideal example for the model. We add a user message containing a question on a similar topic and a response that we consider ideally written for our use case. The model now not only has its pre-existing understanding, but also a working example to guide its response.\n",
    "\n",
    "4. The response: With an example to work with, the assistant provides a succinct and accurate response that's more in-line with our expectations.\n",
    "\n",
    "5. Storing responses: Another common use for providing assistant messages is to store responses. Storing responses means that we can create a conversation history, which we can feed into the model to have back-and-forth conversations. This is exactly what goes on underneath AI chatbots like ChatGPT!\n",
    "\n",
    "6. Building a conversation: To code a conversation, we'll need to create a system so that when a user message is sent, and an assistant response is generated, they are fed back into the messages and stored to be sent with the next user message. Then, when a new user message is provided, the model has the context from the conversation history to draw from. This means that if we introduce ourselves in the first user message, then ask the model what our name is in the second, it should return the correct answer, as it has access to the conversation history. Let's start to code this in Python.\n",
    "\n",
    "7. Coding a conversation: We start by defining some base messages, which can include the system message and any other developer-written examples. Then we can define a list of questions. Here we ask why Python is popular and then ask it to summarize the response provided in one sentence, which requires context on the previous response. Because we want a response for each question, we start by looping over the user_qs list. Next, to prepare the user message string for the API, we need to create a dictionary and add it to the list of messages using the list append method. We can now send the messages to the Chat Completions endpoint and store the response. We extract the assistant's message by subsetting from the API response, converting to a dictionary so it's in the correct format, then adding it to the messages list for the next iteration. Finally, we'll add two print statements so the output is a conversation between the user and assistant written as a script.\n",
    "\n",
    "8. Conversation with an AI: We can see that we were successfully able to provide a follow-up correction to the model's response without having to repeat our question or the model's response.\n",
    "\n",
    "9. Let's practice!: You've learned about the key functionality underpinning many AI-powered chatbots and assistants - time to begin creating your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e9baa213cdf722",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### In-context learning\n",
    "\n",
    "For more complex use cases, the models lack the understanding or context of the problem to provide a suitable response from a prompt. In these cases, you need to provide examples to the model for it to learn from, so-called in-context learning.\n",
    "\n",
    "In this exercise, you'll improve on a Python programming tutor built on the OpenAI API by providing an example that the model can learn from.\n",
    "\n",
    "Here is an example of a user and assistant message you can use, but feel free to try out your own:\n",
    "\n",
    "User → Explain what the min() function does.\n",
    "Assistant → The min() function returns the smallest item from an iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc9af1d020718",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    # Add a user and assistant message for in-context learning\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful Python programming tutor.\"},\n",
    "        ____,\n",
    "        ____,\n",
    "        {\"role\": \"user\", \"content\": \"Explain what the type() function does.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dce24fce23645",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Creating an AI chatbot\n",
    "\n",
    "An online learning platform called Easy as Pi that specializes in teaching math skills has contracted you to help develop an AI tutor. You immediately see that you can build this feature on top of the OpenAI API, and start to design a simple proof-of-concept (POC) for the major stakeholders at the company. This POC will demonstrate the core functionality required to build the final feature and the power of the OpenAI's GPT models.\n",
    "\n",
    "Example system and user messages have been provided for you, but feel free to play around with these to change the model's behavior or design a completely different chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b695f11242048c82",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"}]\n",
    "user_msgs = [\"Explain what pi is.\", \"Summarize this in two bullet points.\"]\n",
    "\n",
    "for q in user_msgs:\n",
    "    print(\"User: \", q)\n",
    "\n",
    "    # Create a dictionary for the user message from q and append to messages\n",
    "    user_dict = {\"role\": ____, \"content\": ____}\n",
    "    messages.append(____)\n",
    "\n",
    "    # Create the API request\n",
    "    response = client.____(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        ____,\n",
    "        max_tokens=100\n",
    "    )\n",
    "\n",
    "    # Convert the assistant's message to a dict and append to messages\n",
    "    assistant_dict = {\"role\": ____, \"content\": ____}\n",
    "    ____\n",
    "    print(\"Assistant: \", response.choices[0].message.content, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8280dd082136a9cb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e689e70a93086d28",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "api_key"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
