{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c7c6063-3393-4a95-b9f6-e41bf3ceee0a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efec3b43-ff5b-4cba-b0bc-75701ec7b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from typing import List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c133d10-081b-43a9-bf0f-5063bb1b0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85079ff5-7209-4ec2-a92a-ec9247162bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'D:\\users\\trenton\\Dropbox\\PythonProjects\\DataCamp\\functions')\n",
    "\n",
    "from multilabel import multilabel_train_test_split\n",
    "from score_sub import score_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace076f-4df9-4283-a268-ab8ca45405b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 700)\n",
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.min_rows', 10)\n",
    "pd.set_option('display.expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5698fbde-4512-4417-97aa-bb18e8d8d17b",
   "metadata": {},
   "source": [
    "# Saving data from DataCamp to a local csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2bf183-f310-4383-9a58-4537e197982f",
   "metadata": {},
   "source": [
    "```python\n",
    "# output the data to a format that can be copied and pasted into notepad++\n",
    "# find all non and replace with np.nan\n",
    "# copy all of lists and replace ... in data = np.array([...])\n",
    "# print the columns names df.columns.tolist() and then create a variable in a notebook\n",
    "# create a dataframe with df = pd.DataFrame(data=data, columns=columns)\n",
    "\n",
    "# create the dataframe in the DataCamp console\n",
    "df = pd.read_csv('TrainingData.csv')  # do not specify index_col=0\n",
    "\n",
    "# print each row as a list with a comma at the end and then copy all of the lists into notepad++\n",
    "for r in df.to_numpy():\n",
    "    print(list(r), ',')\n",
    "\n",
    "# print the column names and copy them\n",
    "df.columns.tolist()\n",
    "\n",
    "# in jupyterlab\n",
    "data = np.array([...])  # replace ... with the cleaned lists from notepad++\n",
    "columns = ...  # pasted from DataCamp console\n",
    "\n",
    "df = pd.DataFrame(data=data, columns=columns)\n",
    "\n",
    "df.to_csv('TrainingData.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3731136-2d6e-4986-b2ae-34fa1744790c",
   "metadata": {},
   "source": [
    "# Exploring the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f73a14f-5566-47e6-83f2-44da71584c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/2024-01-19_school_budgeting_with_maching_learning_in_python/TrainingData.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aff791-1d21-427b-a658-1028258ad240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563556b0-7a29-4d6b-b556-e9ac8376a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978f84ff-4bc2-49b6-8d96-8b41011bd643",
   "metadata": {},
   "source": [
    "## Summarize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffeeb68-9cba-42b1-9ea3-e5e514aa40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e48c25f-deb4-411b-b8e3-cff93f10de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the histogram\n",
    "plt.hist(df['FTE'].dropna(), ec='k')  # .dropna() isn't needed\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Distribution of %full-time \\n employee works')\n",
    "plt.xlabel('% of full-time')\n",
    "plt.ylabel('num employees')\n",
    "\n",
    "# Display the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae073d7-a2cf-4215-a253-efaad52c4cac",
   "metadata": {},
   "source": [
    "**The high variance in expenditures makes sense (some purchases are cheap some are expensive). Also, it looks like the FTE column is bimodal. That is, there are some part-time and some full-time employees.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4152c6-4d74-44dc-8764-34b7703bffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21a4285-a60d-439e-a0bb-4d4e7cbdd00c",
   "metadata": {},
   "source": [
    "## Encode the Labels as Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed5b8b-46e5-4c58-8178-d0df14ba9d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type', 'Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status']\n",
    "categorize_label = lambda x: x.astype('category')\n",
    "df[LABELS] = df[LABELS].apply(categorize_label)\n",
    "print(df[LABELS].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f438ff-2132-487b-a4fc-47d2fc318245",
   "metadata": {},
   "source": [
    "## Counting Unique Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa046f05-270d-4ba9-8d7e-bbcf6c4edfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_labels = df[LABELS].apply(pd.Series.nunique)\n",
    "ax = num_unique_labels.plot(kind='bar', ylabel='Number of Unique Categories', xlabel='Column Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa6ec4b-6f90-4a2c-820e-0adaeca72b2f",
   "metadata": {},
   "source": [
    "## `def compute_log_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe98be5-ce50-4caf-a513-1d2898cd8a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_loss(predicted: Union[float, List[float]], actual: [int, List[int]], eps: float=1e-14) -> float:\n",
    "    \"\"\"\n",
    "    Computes the logarithmic loss between predicted and \n",
    "    actual when these are 1D arrays\n",
    "\n",
    "    :param predicted: The predicted probabilities as floats between 0-1\n",
    "    :param actual: The actual binary labels. Either 0 or 1\n",
    "    :param eps (optional): log(0) is inf, so we need to offset our\n",
    "                           precidted values slightly by eps from 0 to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    predicted = np.clip(predicted, eps, 1 - eps)\n",
    "    loss = -1 * np.mean(actual * np.log(predicted)\n",
    "                        + (1 - actual)\n",
    "                        * np.log(1 - predicted))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23552926-321e-4bf9-9669-7068bc1e60b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(0.85, 1), (0.99, 0), (0.51, 0)]\n",
    "\n",
    "for p, y in data:\n",
    "    print(compute_log_loss(p, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd275576-9af3-40e1-9d3a-f97860f3a316",
   "metadata": {},
   "source": [
    "**Lowest: A, Middle: C, Highest: B**\n",
    "\n",
    "**Of the two incorrect predictions, B will have a higher log loss because it is confident and wrong.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deea260-0aea-4c79-ba4e-b4815d7559fa",
   "metadata": {},
   "source": [
    "## Computing log loss with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e9931b-c32a-43e9-b7c2-53fd2010b57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_confident = np.array([0.95, 0.95, 0.95, 0.95, 0.95, 0.05, 0.05, 0.05, 0.05, 0.05])\n",
    "correct_not_confident = np.array([0.65, 0.65, 0.65, 0.65, 0.65, 0.35, 0.35, 0.35, 0.35, 0.35])\n",
    "wrong_not_confident = np.array([0.35, 0.35, 0.35, 0.35, 0.35, 0.65, 0.65, 0.65, 0.65, 0.65])\n",
    "wrong_confident = np.array([0.05, 0.05, 0.05, 0.05, 0.05, 0.95, 0.95, 0.95, 0.95, 0.95])\n",
    "actual_labels = np.array([1., 1., 1., 1., 1., 0., 0., 0., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cbe99d-ee7b-4b22-823a-5fb204bba355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print log loss for 1st case\n",
    "correct_confident_loss = compute_log_loss(correct_confident, actual_labels)\n",
    "print(\"Log loss, correct and confident: {}\".format(correct_confident_loss)) \n",
    "\n",
    "# Compute log loss for 2nd case\n",
    "correct_not_confident_loss = compute_log_loss(correct_not_confident, actual_labels)\n",
    "print(\"Log loss, correct and not confident: {}\".format(correct_not_confident_loss)) \n",
    "\n",
    "# Compute and print log loss for 3rd case\n",
    "wrong_not_confident_loss = compute_log_loss(wrong_not_confident, actual_labels)\n",
    "print(\"Log loss, wrong and not confident: {}\".format(wrong_not_confident_loss)) \n",
    "\n",
    "# Compute and print log loss for 4th case\n",
    "wrong_confident_loss = compute_log_loss(wrong_confident, actual_labels)\n",
    "print(\"Log loss, wrong and confident: {}\".format(wrong_confident_loss)) \n",
    "\n",
    "# Compute and print log loss for actual labels\n",
    "actual_labels_loss = compute_log_loss(actual_labels, actual_labels)\n",
    "print(\"Log loss, actual labels: {}\".format(actual_labels_loss)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71d5d17-738b-4d7a-837c-5349702dc891",
   "metadata": {},
   "source": [
    "**Log loss penalizes highly confident wrong answers much more than any other type. This will be a good metric to use on your models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3842af-d395-4ab3-8d59-1ba934a936ef",
   "metadata": {},
   "source": [
    "# Creating a simple first model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e356e-6dd2-49e2-9a4a-decbd15701d5",
   "metadata": {},
   "source": [
    "## It's time to build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a257a0-cfb6-4acb-b6f6-a5555a644d61",
   "metadata": {},
   "source": [
    "```python\n",
    "data_to_train = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "labels_to_use = pd.get_dummies(df[LABELS])\n",
    "\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(data_to_train, labels_to_use, size=0.2, seed=123)\n",
    "\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "clf.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cdde3a-ed94-48f6-935e-718d56ecd376",
   "metadata": {},
   "source": [
    "## Setting up a train-test split in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe20bd07-e061-42df-9d1b-b9828dcd499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLUMNS = df.select_dtypes(include='number').columns\n",
    "\n",
    "# Create the new DataFrame: numeric_data_only\n",
    "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "\n",
    "# Get labels and convert to dummy variables: label_dummies\n",
    "label_dummies = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,\n",
    "                                                               label_dummies,\n",
    "                                                               size=0.2, \n",
    "                                                               seed=123)\n",
    "\n",
    "# Print the info\n",
    "print(\"X_train info:\")\n",
    "print(X_train.info())\n",
    "print(\"\\nX_test info:\")  \n",
    "print(X_test.info())\n",
    "print(\"\\ny_train info:\")  \n",
    "print(y_train.info())\n",
    "print(\"\\ny_test info:\")  \n",
    "print(y_test.info()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c77394-64ca-4471-90a8-73cd2bd38f14",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41707c4f-006f-48fc-9847-1f6cd6789a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classifier: clf\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da410827-3364-45df-94ed-125da1a6750c",
   "metadata": {},
   "source": [
    "**The good news is that your workflow didn't cause any errors. The bad news is that your model scored the lowest possible accuracy: 0.0! But hey, you just threw away ALL of the text data in the budget. Later, you won't. Before you add the text data, let's see how the model does when scored by log loss.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae519164-080b-4993-a82e-80a218b78500",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc45dec5-f913-47f5-94aa-1a8b1fbcdcda",
   "metadata": {},
   "source": [
    "```python\n",
    "houldout = pd.read_csv('HoldoutData.csv', index_col=0)\n",
    "holdout = holdout[NUMERIC_COLUMNS].fillna(-1000)\n",
    "predictions = clf.predict_proba(holdout)\n",
    "```\n",
    "\n",
    "- Using `.predict()`\n",
    "  - would result in an output of 0 or 1\n",
    "  - Log loss penalized being confident and wrong\n",
    "  - Worse performance compared to `.predict_proba()`\n",
    "\n",
    "- Format and submit predictions\n",
    "```python\n",
    "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS], prefix_sep='__').columns, index=houldout.index, data=predictions)\n",
    "prediction_df.to_csv('predictions.csv')\n",
    "score = score_submission(pred_path='predictions.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a60c0-2f7f-479e-81de-a7dffaf8515b",
   "metadata": {},
   "source": [
    "## Use your model to predict values on holdout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f53fd-8846-4a21-9605-90ca91ca08f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classifier: clf\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "# Fit it to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Load the holdout data: holdout\n",
    "holdout = pd.read_csv('data/2024-01-19_school_budgeting_with_maching_learning_in_python/HoldoutData.csv', index_col=0)\n",
    "\n",
    "# Generate predictions: predictions\n",
    "NUMERIC_COLUMNS = holdout.select_dtypes(include='number').columns\n",
    "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f6251-2ebf-403c-9e64-6192cf7ea705",
   "metadata": {},
   "source": [
    "## Writing out your results to a csv for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69f822a-ed43-4139-ada1-390c9fab0bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format predictions in DataFrame: prediction_df\n",
    "# pd.get_dummies(df[LABELS], ...) is correctly using df, not holdout\n",
    "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS], prefix_sep='__').columns,\n",
    "                             index=holdout.index,\n",
    "                             data=predictions)\n",
    "\n",
    "# Save prediction_df to csv\n",
    "prediction_df.to_csv('data/2024-01-19_school_budgeting_with_maching_learning_in_python/predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2ce94e-6704-40d2-a91f-5676f30a6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires functions and variables from https://goodboychan.github.io/python/datacamp/machine_learning/2020/06/05/01-School-Budgeting-with-Machine-Learning-in-Python.html\n",
    "# BOX_PLOTS_COLUMN_INDICES, def _multi_multi_log_loss, and def score_submissio\n",
    "\n",
    "# Submit the predictions for scoring: score\n",
    "score = score_submission(pred_path='data/2024-01-19_school_budgeting_with_maching_learning_in_python/predictions.csv', holdout_path='data/2024-01-19_school_budgeting_with_maching_learning_in_python/TestSetLabelsSample.csv')\n",
    "\n",
    "# Print score\n",
    "print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68f7382-5f4b-4f01-a69a-dda406e6b18d",
   "metadata": {},
   "source": [
    "**Even though your basic model scored 0.0 accuracy, it nevertheless performs better than the benchmark score of 2.0455. You've now got the basics down and have made a first pass at this complicated supervised learning problem. It's time to step up your game and incorporate the text data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4931b14-f5ce-4508-bccd-a4ef68c7e694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
